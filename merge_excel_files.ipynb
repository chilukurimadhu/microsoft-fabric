{"cells":[{"cell_type":"code","source":["#%run \\common_functions"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d90cb27-d64c-4232-8e7a-97b0690de861"},{"cell_type":"code","source":["\n","#ctx=convert_ctx_object(site_url,username,password)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"51dbad50-bc87-45d2-9320-89e490041317"},{"cell_type":"code","source":["def merge_df(common_substring,ctx,folder_url,sheet_name,Headers):\n","    try:\n","            \n","        # Get all files in the folder\n","        all_files = list_files_in_folder(ctx, folder_url)\n","\n","        # Filter files with the common substring\n","        filtered_files = [file_url for file_url in all_files if common_substring in file_url and file_url.endswith(\".xlsx\")]\n","        print(f\"Filtered files: {filtered_files}\")\n","\n","        # Initialize an empty pandas DataFrame for merging\n","        merged_pandas_df = pd.DataFrame()\n","\n","        # Loop through filtered files and merge their data\n","        for file_url in filtered_files:\n","            # Fetch data from each file\n","            df = fetch_excel_to_dataframe(ctx, file_url, sheet_name, None)\n","            df=df[df[0].notnull()]\n","\n","            # Insert an empty row between the first and second rows\n","            if len(df) > 1:  # Ensure the DataFrame has at least two rows\n","                empty_row = pd.DataFrame([[\"\"] * len(df.columns)], columns=df.columns)\n","                df = pd.concat([df.iloc[:1], empty_row, df.iloc[1:]] ,ignore_index=True)#\n","\n","            # Merge with the consolidated DataFrame\n","            merged_pandas_df = pd.concat([merged_pandas_df, df], ignore_index=True)\n","\n","        # Ensure all data is treated as strings\n","        merged_pandas_df = merged_pandas_df.astype(str)\n","\n","        return merged_pandas_df\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"b710c5fa-245d-41d4-b3ad-39ab12211f6d","normalized_state":"finished","queued_time":"2025-01-02T16:34:40.6097028Z","session_start_time":null,"execution_start_time":"2025-01-02T16:35:04.9535635Z","execution_finish_time":"2025-01-02T16:35:05.3270133Z","parent_msg_id":"2a0b8fd2-56d6-4709-b99b-f51f655b3b2c"},"text/plain":"StatementMeta(, b710c5fa-245d-41d4-b3ad-39ab12211f6d, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a3a00582-37dc-4e38-8fae-1c55be6ff283"},{"cell_type":"code","source":["\"\"\"if logic=='write_to_sharepoint':\n","    pandas_df=merge_df(common_substring,ctx,f'{folder_url}','Forecast',None)\n","    display(pandas_df)\n","    #write_to_sharepoint(pandas_df,'Multiple Long Term Forecast Template 20241212.csv',False)\n","    print('WRITTEN TO SHAREPOINT')\n","    notebookutils.notebook.exit(\"file has been written to sharepoint\")\"\"\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2d50ecf0-73cb-46ba-9f54-14c8b92ac7be"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"7200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}